{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Details\n",
    "\n",
    "Student name:\n",
    "\n",
    "Student ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Description\n",
    "\n",
    "In this task, we will use NSL-KDD dataset. We will use the NSL-KDD dataset to do multi-class classification. This dataset is quite large, and the training time can be quite long if you use the whole thing so that we will use just 20% of the dataset. If you completed the network security task in the previous activity (i.e., if you are in the BSc in CyberSecurity), you will have seen this already. For those that have not seen this dataset before, you will also be dealing with many more dimensions than you have done up to now, but you will see that the machine learning techniques we have employed up to now scale nicely to many dimensions.\n",
    "\n",
    "The aim of the NSL-KDD dataset is to enable training a machine learning algorithm to identify different types of cyber attacks based on network trafiic features. The different attacks can be: denial-of-service (dos), Remote to user (r2l), probing attack (probe), User-to-Root (U2R). I hope this means something to the CyberSecurity cohort. For the rest of us, don't worry, we can just see it as a generic classification task.\n",
    "\n",
    "The data is already split into training and testing. It also contains a mix of different types of features - categorical, binary, and numerical features. However, in this task, we are going to investigate only the numerical features. So, in the code just below, I have stripped out all of the non-numerical features, and provide you with the numpy arrays `train_X`, `train_Y`, `test_X`, and `test_Y`.\n",
    "\n",
    "Our aim will be to use the available data to train an algorithm to predict the type of attack that is occurring. We will then see if we can get similar performance by using fewer features. Undoubtedly, unless there is a feature that has no influence *at all* on the output, we will see *some* degradation in performance. However, as discussed in the material, there are significant gains to be made by using fewer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'KDDTest_CE4317.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m####################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# CODE PROVIDED\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39m# Read the data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mKDDTest_CE4317.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mKDDTrain_CE4317.csv\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Differentiating between nominal, binary, and numeric features\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Note, we only need to do this for the train data, as the train and test have the same feature names (of course)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KDDTest_CE4317.csv'"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# CODE PROVIDED\n",
    "\n",
    "# This code is a little bit complicated, and I don't want you to get bogged down in reading from csv files.\n",
    "# This code reads from the CSV files, and creates the training and test sets for both binary and multi-class\n",
    "\n",
    "# Read the data\n",
    "import pandas as pd\n",
    "test_df = pd.read_csv('KDDTest_CE4317.csv', header=0)\n",
    "train_df = pd.read_csv('KDDTrain_CE4317.csv', header=0)\n",
    "\n",
    "# Differentiating between nominal, binary, and numeric features\n",
    "# Note, we only need to do this for the train data, as the train and test have the same feature names (of course)\n",
    "col_names = train_df.columns.values    \n",
    "\n",
    "nominal_idx = [1, 2, 3]\n",
    "binary_idx = [6, 11, 13, 14, 19, 20]\n",
    "numeric_idx = list(set(range(40)).difference(nominal_idx).difference(binary_idx))\n",
    "\n",
    "numeric_cols = col_names[numeric_idx].tolist()   # The columns that have numerical features\n",
    "\n",
    "train_Y = train_df['attack_category']\n",
    "test_Y = test_df['attack_category']\n",
    "\n",
    "# In this case, we are only going to use the numeric columns for our predictions\n",
    "train_X = train_df[numeric_cols]   \n",
    "test_X = test_df[numeric_cols]\n",
    "# print(train_X.columns.values)\n",
    "# num_missing = (train_X[[1,2,3,4,5]] == 0).sum()\n",
    "# print(num_missing)\n",
    "\n",
    "print(\"shape of train_X\", train_X.shape)\n",
    "print(\"shape of test_X\", test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Let's have a quick look at what some of the samples look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at the data\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look quickly at how many samples in each attack category we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_attack_cats = train_df['attack_category'].value_counts()\n",
    "test_attack_cats = test_df['attack_category'].value_counts()\n",
    "train_attack_cats.plot(kind='barh', figsize=(10,5), fontsize=15)\n",
    "plt.xlabel(\"Number of samples\", fontsize=20)\n",
    "plt.ylabel(\"Attack category\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Task 1- Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Part 1: Support Vector Classification\n",
    "\n",
    "Here, we will use Support Vector Classification to predict the type of network attack that is occurriing, given a set of features. We will use a simple linear SVM Classification, and use the default parameters, as we're not investigating the properties of SVM, but rather the properties of data.\n",
    "\n",
    "#### Task:\n",
    "1. Apply the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to the training and test data. Remember, train on the `*_train` data, but apply to both the `*_train` and the `*_test` data\n",
    "1. Train a linear Support Vector Classification, using [`sklearn`'s `svm.SVC` class](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). All parameters can be left at default except `kernel='linear'`\n",
    "1. Predict the category of the network attack\n",
    "1. Print the [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and the [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "1. Discuss the confusion matrix briefly\n",
    "\n",
    "You should see that the linear SVM Classification isn't too bad. You should get an accuracy around 0.73\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<span style=\"color:red\">Insert your code below</span>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for calculating Standard Scaler for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "####################################\n",
    "# Your code here\n",
    "def scaleData(X_train, X_test) :\n",
    "    # scaling train data and test data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "    # apply the fitted standard scaler to test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "####################\n",
    "# YOUR CODE\n",
    "\n",
    "# using standard scaler to fit the train and test data by calling the method\n",
    "X_train_scaled, X_test_scaled = scaleData(train_X, test_X)\n",
    "\n",
    "# using svm.SVC with linear kernel\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "# train the model and predict the y_pred\n",
    "clf.fit(X_train_scaled, train_Y)\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"The accuracy score with svm.SVC is\", accuracy_score(test_Y, y_pred)) # calculating accuracy\n",
    "confusion_matrix(test_Y, y_pred) # display the confusion matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<span style=\"color:red\">Insert your text answers below</span>.  \n",
    "\n",
    "The confusion matrix below tells us where the model went wrong(how our model is performing). The confusion matrix provides the values it \"confused\" with the correct results or predicted as result. It offers us the count of inaccurate predictions it generated in comparison to the actual result values. The sum of all the values in the confusion matrix provides us the same result as the number of test samples(22544). Sum of the values in the diagonal provides the correct predictions against the test results ie. the (sum of diagonal)/(total count of test values) would provide us the accuracy((9085+5521+1864+2+5)/22544 = 0.7308818310858765). However, the other values are the mismatched values.\n",
    "\n",
    "The confusion Matrix plotted below shows values with a color bar which illustrates the color it would presume according to the number of values.It goes from white to red as the number of observations increase. The higher the result, the darker the value eg. 9085 for benign shows this. On the X it has predicted values and the true values on the Y. Here we can find the result \"benign\" predicted correcly 9085 times, however incorrectly as dos, probe and r2l as 472, 150 and 4 times, which would lead to the decrease in accuracy. This can be read as for benign class it predicted benign 9085(correctly) out of all the data and incorrectly as dos as 472 times and probe and r2l as 150 and 4 times respectively\n",
    "\n",
    "<u>Note</u>: Here I have altered the color to white and red, where darker red corresponds to more number of observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(9085+5521+1864+2+5)/22544 # sum of the diagonal values divided by the total count of test samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method plotConfusionMatrix to plot confusion matrix which is used later as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotConfusionMatrix(test_Y, y_pred, labels) :\n",
    "    \n",
    "    # Displaying the confusion matrix\n",
    "    cm = confusion_matrix(test_Y, y_pred, labels=labels)\n",
    "    matrix = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "    matrix.plot(cmap=plt.cm.Reds) # changing the color from the default to red inorder for a better visualization\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(test_Y, y_pred, clf.classes_)  # plot confusion Matrix using the method defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Part 2: Feature Variance\n",
    "\n",
    "Feature variance is a rather simple way of predicting if a given feature will have influence on the outcome of a trained model. The principle is that, if a feature has low variance, it cannot have much influence on the model prediction. As an extreme, if we have a variance of 0 in a feature across all samples, i.e. we have the same value for this feature in all samples, then this feature cannot be used to distinguish samples and is useless as a predictor.\n",
    "\n",
    "However, the converse is not true. A high variance in a feature does not necessarily mean that it is a good predictor. You could imagine a feature that just contains noise with high amplitude. It might have high variance, but is meaningless. Or you can have a feature that has high variance but no influence on the outcome. For example, would hair length in cm have any influence on a baseball players salary? That said, variance can be a useful measure of the strength of a predictor.\n",
    "\n",
    "The variance of a set of features is given by:\n",
    "\n",
    "$$\n",
    "S_{i}^{2} = \\frac{\\sum_{j=1}^n\\left(\\textbf{X}_{i,j} - \\overline{\\textbf{X}}_i \\right)^2}{n - 1}\n",
    "$$\n",
    "\n",
    "where $\\textbf{X}_{i,j}$ is the $j$'th sample of the $i$'th feature, $\\overline{\\textbf{X}}_i$ is the mean of all the samples of the $i$'th feature, and $n$ is the total number of samples.\n",
    "\n",
    "Variance thresholding doesn't examine the relationship between the feature $\\textbf{X}$ and the output $\\textbf{y}$. This has the disadvantage that you can't test if the feature actually has an influence on the output. However, even though in this case we use it in a supervised learning context, it does mean that we can use variance thresholding for unsupervised learning.\n",
    "\n",
    "#### Notes:\n",
    "1. In Part 1, we used the `StandardScaler` to scale the features. In general, this is good practice, and in the next Task where we look at PCAs, really is even required. The `StandardScaler` will make it so every feature has a variance of 1 (unless the features started out with a variance of 0 to begin with) and a mean of 0. Therefore, features scaled with `StandardScaler` are useless for thresholding on variance, as there is no practical way to distinguish them.\n",
    "2. However, we should not do `VarianceThreshold`ing on just the raw data. Have a look at the values in the training dataset. Some columns will have typically small values. It is the nature of that data, and even though they may have a large influence on the type of attack, they will have a lower variance compared to some of the other columns.\n",
    "3. So we must scale, but not using the `StandardScaler`. Here it is more appropriate to use the [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), which scales all the data to the range 0 to 1 by default (though you can set any range).\n",
    "4. Note that `MinMaxScaler` can also be applied to machine learning algorithms. Just in this case, we want to use the `StandardScaler`. There is, in fact, a [whole suite of other scalers provided by scikit-learn](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html). Each has it's own benefits. In fact, some would say we should use `MinMaxScaler` as the default scaler, and only use `StandardScaler` if we know the distribution of the data is normal.\n",
    "\n",
    "#### Task:\n",
    "1. Fit an instance of the `MinMaxScaler` with the original `X_train` data. You will get a set of data in the range 0 to 1.\n",
    "2. It is not very intuitive what threshold of variance we should use. So it's better to plot the variances of each feature, and then decide if some of the variances are small enough to discard\n",
    "3. Use `np.var` function to calculate the variances of the features (`axis-0`). Plot the variances, and pick a value that might remove 3 or 4 of the features.\n",
    "4. Fit the output of the `MinMaxScaler` using an object of [`sklearn.feature_selection`'s `VarianceThreshold` class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) using this threshold\n",
    "5. `transform` the data that was scaled with the `StandardScaler`. This is an important step. Notice that we use the output of the `MinMaxScaler` to select the features, but the data we want to use is selected from the `StandardScaler`.\n",
    "6. Repeat the steps of Part 1: Train an SVC with the selected features and print the accuracy.\n",
    "7. How does the accuracy compare to the the SVC with no features removed (from Part 1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<span style=\"color:red\">Insert your code below</span>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using bar graph for better comprehensibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "####################################\n",
    "# Your code here\n",
    "# using min max scaler for applying variance thresholding\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaled_train_X = min_max_scaler.fit(train_X)\n",
    "min_max_scaled_train_X = min_max_scaler.transform(train_X)\n",
    "# calculate variances of all the features in the data in order to remove those with less variance\n",
    "variances = np.var(min_max_scaled_train_X, axis=0)\n",
    "\n",
    "plt.bar(range(len(variances)), variances)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Feature Variances')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying thresholding post MinMaxScaling and then StandardScaling the data before classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a threshold value that removes a few of the features\n",
    "threshold = 0.00015 # threshold value of .00015 chosen to remove 5 features which doesn't reduce the accuracy over 2 decimal places\n",
    "sel = VarianceThreshold(threshold)\n",
    "# This ensures that both datasets have the same scaling and feature selection transformations applied to them, leading to better model generalization and potentially higher accuracy.\n",
    "\n",
    "min_max_scaled_test_X = min_max_scaler.transform(test_X)\n",
    "\n",
    "# fitting and transforming the data based on the threshold chosen on train data and transforming the test data\n",
    "thresholded_train_X = sel.fit_transform(min_max_scaled_train_X)\n",
    "thresholded_test_X = sel.transform(min_max_scaled_test_X)\n",
    "\n",
    "print(f\"The number of features to consider by the classifier after thresholding are {thresholded_test_X.shape[1]}\")\n",
    "\n",
    "# Applying Standard Scaler on the data before calling the classifier\n",
    "standard_scaled_train_X, standard_scaled_test_X = scaleData(thresholded_train_X, thresholded_test_X)\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(standard_scaled_train_X, train_Y)\n",
    "y_pred = clf.predict(standard_scaled_test_X)\n",
    "\n",
    "print(\"The accuracy score with svm.SVC is\", accuracy_score(test_Y, y_pred)) # calculating accuracy\n",
    "plotConfusionMatrix(test_Y, y_pred, clf.classes_)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix after removing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(test_Y, y_pred, clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<span style=\"color:red\">Insert your text answers below</span>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the accuracy compare to the the SVC with no features removed (from Part 1)?  \n",
    "The accuracy has reduced by .0006 percent when we have 26 features with a threshold value of 0.00015(accuracy is 0.7302608232789212), whereas, the accuracy obtained with no features removed were 0.7308818310858765. This does not produce a wide impact to the model as it does not make a significant difference to the result produced. Hence we can say that removing 5 features with a low variance does not do much harm to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Part 3: Univariate Feature Selection - `f_classif`\n",
    "\n",
    "Univariate feature selection works by performing statistical tests on each of the features (i.e. on each column in our dataset). There are a [few options provided by `scikit-learn`](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). We will use the [`SelectKBest` functionality](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html), which allows us to pick \"the top\" `K` features per the metric we select. To pick the top features, we will use the [`f_classif` function](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html), as we are doing a classification. `f_classif` uses the ANOVA F-value to determine features to select. More info on ANOVA is available [here](https://datascience.stackexchange.com/questions/74465/how-to-understand-anova-f-for-feature-selection-in-python-sklearn-selectkbest-w).\n",
    "\n",
    "The `SelectKBest` functionality coupled with `f_classif`, will use this score to pick the `K` top features.\n",
    "\n",
    "#### Task:\n",
    "1. Loop over the total count of features (i.e. for variable `k` from 1 to 31)\n",
    "2. Use the [`SelectKBest` class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) with [`f_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html) to pick the top `k` features from our dataset\n",
    "3. Train a new SVM classification with features transformed with the `SelectKBest` object you just created (note: both train and test data have to be transformed)\n",
    "4. Use the `accuracy_score` function to get the accuracy at each iteration\n",
    "5. Repeat steps 2 to 4 for each value of `k`.\n",
    "6. Then plot the accuracy versus number of features in a single plot\n",
    "7. Given this data, discuss the number of features you might use in a final solution? (Use markdown - no wrong answer here)\n",
    "8. How doe the \"best\" accuracy value compare with the SVM before removing any features? \n",
    "\n",
    "this will take a few minutes to run, go get a coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Insert your code below</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Your code here\n",
    "\n",
    "accuracy_scores = []\n",
    "for k in range(1, train_X.shape[1] + 1):\n",
    "    standard_scaled_train_X, standard_scaled_test_X = scaleData(train_X, test_X) # scaling the data to run it faster, as this shouldnt make a considerable difference to the intended target as is clearly evident from the accuracy of the k = 31\n",
    "    select_K_best = SelectKBest(score_func=f_classif, k=k)\n",
    "    select_K_best.fit(standard_scaled_train_X, train_Y)\n",
    "    X_train_K_best = select_K_best.transform(standard_scaled_train_X)\n",
    "    X_test_K_best = select_K_best.transform(standard_scaled_test_X)\n",
    "\n",
    "    # Using SVM for classification with a linear kernel\n",
    "    clf = SVC(kernel='linear')\n",
    "\n",
    "    clf.fit(X_train_K_best, train_Y)\n",
    "    y_pred = clf.predict(X_test_K_best)\n",
    "\n",
    "    accuracy_score_obtained = accuracy_score(test_Y, y_pred)\n",
    "    accuracy_scores.append(accuracy_score_obtained)\n",
    "    print(f\"The accuracy score with SVM.SVC is with k as {k} is \", accuracy_score_obtained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "####################################\n",
    "# Your code here\n",
    "plt.plot(range(1, X_test_K_best.shape[1] + 1), accuracy_scores, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Number of Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<span style=\"color:red\">Insert your question answers below</span>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">The above graph shows the Accuracy vs. Number of Features. It illustrates that the accuracy does not vary much after the consideration of 17 features. This helps us reduce the features for consideration while passing them onto the classifier, which improves the perfomance of the model</span>.\n",
    "\n",
    "7. Given this data, discuss the number of features you might use in a final solution? (Use markdown - no wrong answer here)  \n",
    "    Here we can choose 25 as the maximum number of features required for the model to perform well, as the accuracy lies in the range of 73, which is almost the same as the accuracy value when we take all the 31 features into consideration. Here we can remove 6 features there by making the classifier performant.  \n",
    "\n",
    "    If we need a really performant version of the model then we might even consider reducing the feature count even further to 18, which is nearly half the original size of the features(31), as it would still produce an accuracy of 72.9%  \n",
    "\n",
    "8. How doe the \"best\" accuracy value compare with the SVM before removing any features?  \n",
    "    The best accuracy obtained is 0.7308818310858765 which is identical to the value obtained in Task 1 before removing any of the features from the dataset. This is expected as the accuracy 0.7308818310858765 is what we have when we take all the featues(31) into consideration.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Task 2: Dimensionality Reduction via PCA\n",
    "\n",
    "PCA is one of the most commonly used unsupervised transforms, and one of the most commmon means to manipulate data for machine learning. You touched on the PCA in E-tivity 2, where we investigated linear algebra. Here we will use it to reduce the numbers of features needed for a machine learning algorithm.\n",
    "\n",
    "In the last task, we removed features. The first part, we just used some statistics on the features themselves, in independence of the other features and of the output. Then, we looked at the correlation between features and the output. \n",
    "\n",
    "What PCA does is look at correlations *between features*. If we have high correlation between two or more features, PCA will find vectors in the feature space that best describe all features. It doesn't remove features, rather it creates a new feature space, and projects all samples to this feature space. The basis of the new feature space is a linear combination of the original features. Maybe a bit crudely, you can think of it as combining features.\n",
    "\n",
    "Let's look at an example. Here is the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Ok, so in a small handfull of features, we can spot that (perhaps) `num_root` and `num_compromised` are correlated? Let's plot a few of them that might be correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "ax[0].scatter(train_df['num_compromised'], train_df['num_root']);\n",
    "ax[0].set(xlabel='num_compromised', ylabel='num_root')\n",
    "\n",
    "ax[1].scatter(train_df['srv_serror_rate'], train_df['serror_rate']);\n",
    "ax[1].set(xlabel='srv_serror_rate', ylabel='serror_rate')\n",
    "\n",
    "ax[2].scatter(train_df['srv_rerror_rate'], train_df['rerror_rate']);\n",
    "ax[2].set(xlabel='srv_rerror_rate', ylabel='rerror_rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Note that the last two plots, while there are outliers in the plots (values at 1.0), most of the data lies along the diagonal. Just the drawing doesn't show this well, though it is highly correlated.\n",
    "\n",
    "Yes, we can see that there is some correlation between the features we selected here. We can probably assume that there is a causal relationship between them - CyberSecurity specialists wish to comment?\n",
    "\n",
    "So there is certainly some redundancy here. And likely there are more hidden correlations that we don't know about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Task\n",
    "\n",
    "On to this weeks task. We will perform PCA on the data, before training a linear SVM, and explore some more properties of it, and how it affects the machine learning algorithm.\n",
    "\n",
    "1. Run [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) on the standard scaled data. Initially, set the desired variance to keep to 95% (`n_components=0.95`, all other parameters set to default)\n",
    "2. Train a Support Vector Classification on the PCA reduced data. As with Task 1, use a linear SVM and keep all other parameters as default\n",
    "3. Try 99% and 90%\n",
    "4. Play around with desired variance to see if you can reduce the number of features while maintaining an accuracy close to the original dataset above\n",
    "\n",
    "Discuss the following points, and compare with the previous task in this e-tivity:\n",
    "\n",
    "1. How many new features are there after the PCA?\n",
    "2. Discuss the \"goodness\" of the model, compared with the one without scaling (from Task 1), by comparing the accuracy\n",
    "3. How about if we set the variance to 99%? And how about 99.9%?\n",
    "4. Can you get better accuracy with fewer features using PCA compared to dropping the features from Task 1?\n",
    "\n",
    "Note that the parameter `n_components` of `PCA` can take either a real value between 0 and 1, in which case it will pick the number of components that maintains that level of variance in the samples, or it can take an integer value, in which case it will keep that number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<span style=\"color:red\">Insert your code below</span>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a method to calculate PCA and plot the accuracy to be used later with different values of variance and to plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "####################################\n",
    "# Your code here\n",
    "\n",
    "def applyPCA(variance) :\n",
    "    # Applying PCA on the standard scaled data from above with the variance at 95%\n",
    "    pca = PCA(n_components=variance)\n",
    "\n",
    "    X_train_scaled_pca = pca.fit(X_train_scaled)\n",
    "    X_train_scaled_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "    X_test_scaled_pca = pca.transform(X_test_scaled)\n",
    "    print(f\"Number of features in standard scaled Test data after PCA with variance {variance*100}: \", X_test_scaled_pca.shape[1])\n",
    "\n",
    "    # using svm.SVC with linear kernel\n",
    "    clf = SVC(kernel='linear')\n",
    "\n",
    "    clf.fit(X_train_scaled_pca, train_Y)\n",
    "    y_pred = clf.predict(X_test_scaled_pca)\n",
    "    print(f\"The accuracy score with svm.SVC after PCA with variance of {variance*100}% is\", accuracy_score(test_Y, y_pred), \"\\n\") # calculating accuracy\n",
    "    plotConfusionMatrix(test_Y, y_pred, clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "####################################\n",
    "# Your code here\n",
    "print(\"The accuracy score with svm.SVC is\", accuracy_score(test_Y, y_pred)) # calculating accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired Variance 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_95 = 0.95\n",
    "applyPCA(variance_95) # apply PCA with variance 95%\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired Variance 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_99 = 0.99\n",
    "applyPCA(variance_99) # apply PCA with variance 99%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired Variance 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_90 = 0.90\n",
    "applyPCA(variance_90) # apply PCA with variance 90%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired Variance 99.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_999 = 0.999\n",
    "applyPCA(variance_999) # apply PCA with variance 99.9%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly added features reduce the accuracy due to addition of new features which throws the model to a little less inaccurate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired Variance 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_10 = 0.10\n",
    "applyPCA(variance_10) # apply PCA with variance 10%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired Variance 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_20 = 0.20\n",
    "applyPCA(variance_20) # apply PCA with variance 20%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired Variance 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_30 = 0.30\n",
    "applyPCA(variance_30) # apply PCA with variance 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Insert your text answers below</span>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many new features are there after the PCA?\n",
    "\n",
    "    After the application PCA to the question, we see a reduction in number of features as intended. With a higher value for Desired Variance such as 99.9% we see only 3 of the features are removed from the dataset. However, on reducing the value of desired variance we observe the reduction in number of features as well. This is based on the degree of the variance in dataset we possess. In the above dataset, we see a considerable reduction of features(over half) when we decrease the variance to just 90% and continues to decrease as we reduce the desired variance percentage.  <u>Note</u>: the number of features taken for the classifer are 2 when the desired variance is 30%.\n",
    "\n",
    "\n",
    "2. Discuss the \"goodness\" of the model, compared with the one without scaling (from Task 1), by comparing the accuracy\n",
    "    Even though in the Task1 we were asked to scale the model, the model stays relevant for classifcation due to very small variations of accuracy from the one where we did not have any removal of features(Task 1). As we can see from the data above, when the desired variance is 99.9%, we observe an accuracy of 0.7298172462739532(72.98%) which is close enough to 0.7308818310858765(73.09%) that we observed without using PCA or removal of any features. Even with 90% desired variance, we observe an accuracy rating of 72.63% which is only 0.4 percent less than the accuracy obtained when we considered all the features. The advantage of this is that we only consider 16 features, which would make a considerable difference in performance as it is nearly half the feature size. Hence I would say that the model is considerably \"good\" when compared to the one without removal of features.\n",
    "\n",
    "3. How about if we set the variance to 99%? And how about 99.9%?  \n",
    "    When the variance is set as 99% we see a drop in number of features considered to 23 and the accuracy drops to 0.7301721078779276. However, with the variance of 99.9% we see the number of features considered are 28, but interestingly the accuracy of the model drops to 0.7298172462739532. This may be because we are considering one too many features with outlier data in them. The newly added features ie. the 5 features we are considering for the 99.9 percent desired variance, could be having outlier data which is throwing the model off balance. This is a key point in determining a good model as to how we take the features and how many of those features are critical for the model to perform well.\n",
    "\n",
    "4. Can you get better accuracy with fewer features using PCA compared to dropping the features from Task 1?  \n",
    "    As we can see from the result of the code below the accuracy obtained with dropping 12 features is 72.85%. Using PCA if we only consider 19 features(dropping 12 features) we see an accuracy of 72.78%(from above where the desired variance is 95%). Hence, numerically PCA does not yield a better result for this dataset, however, PCA would be useful where dimensionality reduction is key. Even for this data the difference in magnitude of the accuracy is negligible. However, this has be done only with domains and features where we know it would not cause any harm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to answer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0085 # threshold value of .00015 chosen to remove 5 features which doesn't reduce the accuracy over 2 decimal places\n",
    "sel = VarianceThreshold(threshold)\n",
    "# This ensures that both datasets have the same scaling and feature selection transformations applied to them, leading to better model generalization and potentially higher accuracy.\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaled_train_X = min_max_scaler.fit_transform(train_X)\n",
    "min_max_scaled_test_X = min_max_scaler.transform(test_X)\n",
    "\n",
    "thresholded_train_X = sel.fit_transform(min_max_scaled_train_X)\n",
    "thresholded_test_X = sel.transform(min_max_scaled_test_X)\n",
    "\n",
    "print(f\"The number of features to consider by the classifier after thresholding are {thresholded_test_X.shape[1]}\")\n",
    "\n",
    "standard_scaled_train_X, standard_scaled_test_X = scaleData(thresholded_train_X, thresholded_test_X)\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(standard_scaled_train_X, train_Y)\n",
    "y_pred = clf.predict(standard_scaled_test_X)\n",
    "\n",
    "print(\"The accuracy score with svm.SVC is\", accuracy_score(test_Y, y_pred)) # calculating accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 : Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection with Recursive Feature Elimination(RFE)\n",
    "\n",
    "RFE is a feature selection technique used to locate key featueres in a dataset. RFE is used most commonly with SVMs. RFE works by estimating the importance of each of the features associated with the testdata and provides ranking to the features based on their importance. The next step involves removing the least important feature with the lowest importance or highest value for rank. Then we build a model using the remaining features and the above steps are run in a loop until the desired Number of features are obtained. RFE observes interactions between features in order to determine the rank of features. This is an advantage with RFE.   \n",
    "\n",
    "RFE is a wrapper feature selection algorithm as the core of the machine learning algorithm would be different and its used to obtain the best features. It often uses filter based feature selection internally. The ranking is determined either by machine learning algorithms used in the core(eg. decision trees) or by using statistical methods.\n",
    "\n",
    "An example of RFE\n",
    "\n",
    "// define the method  \n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=3)  \n",
    "//fit the model  \n",
    "rfe.fit(X, y)  \n",
    "//transform the data  \n",
    "X, y = rfe.transform(X, y)  \n",
    "\n",
    "Here the estimator argument is the machine learning model we choose to evaluate the importance of features and the n_features_to_select is the number of features we want to select in the end. RFE is often used with k-fold cross-validation to prevent data leakage\n",
    "\n",
    "Besides the obvious reasons of reducing the training time and improving the model performance, it helps the model to focus on most significant predictors and derive a relationship between the features. It also aids in the systematic testing of impact of each of the features on the model. It can handle interactions between features and hence is suitable for complex datasets. It can also handle correlation between several features, eventhough not preferred due to its computational complexity as one of the factors.\n",
    "\n",
    "### Things to keep in mind while working with RFE\n",
    "1. Number of features chosen should be in such a way that it keeps a balance between power and complexity of the features.  \n",
    "2. Setting the number of cross-validation folds which can help in reduction of overfitting and thereby improve the generalization of the model.\n",
    "\n",
    "However, it is computationally expensive for large datasets.\n",
    "## Dimensionality Reduction using Isomap\n",
    "\n",
    "Isomap is used when data is non-linear(correlation between features is non-linear). If we extrapolate it on to a linear plane we might loose some of the critical information. This can happen with geometric structures of data and Isomap preserves the geodesic distance. Isomap uses geodesic distance along with K nearest neigbours to create a similarity matrix for eigen value decomposition. It uses local information to create global similarity matrix. The algorithm uses Eucledian distance to prepare nearest neighbour graph and then approximates geodesic distance between two points by measuring the shortest distance between the two points. Hence it gets the global as well as local structure of the dataset on to a lower dimension.\n",
    "\n",
    "\n",
    "It comes as a part of manifold learning. A manifold can be thought of as a surface of any shape ie. it doesn't need to fit on a plane. While PCA can be used to project data onto a lower dimensional surface(linear), Isomap works without this as it can work on data on any surface, which the PCA cannot do.\n",
    "The data points here can be seen as samples from a lower dimensional manifold that is embedded in a higher dimensional space. Other algorithms that can do this are Locally Linear Embedding, Laplacian Eigen maps etc.\n",
    "\n",
    "Page 494, Applied Predictive Modeling, 2013.  \n",
    "https://machinelearningmastery.com/rfe-feature-selection-in-python/  \n",
    "https://prateekvjoshi.com/2014/06/21/what-is-manifold-learning/    \n",
    "https://blog.paperspace.com/dimension-reduction-with-isomap/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
